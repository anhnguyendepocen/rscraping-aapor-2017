twitter_token
library(rtweet)
team_rstats <- search_tweets("#rstats", n = 500)
tweets_datascience <- search_tweets("#datascience", n = 500)
head(v)
head(tweets_datascience)
names(v)
names(tweets_datascience)
tweets_datascience$text
tweets_datascience <- search_tweets("#AAPOR", n = 10)
tweets_aapor <- search_tweets("#AAPOR", n = 10)
tweets_aapor
names(tweets_aapor)
dim(tweets_aapor)
?search_tweets
tweets_aapor$text[1]
tweets_aapor$text[1:3]
tweets_aapor$text[1:5]
tweets_aapor$text[1:3]
names(tweets_aapor)
names(tweets_aapor)
tweets_aapor <- search_tweets("#AAPOR", n = 100)
tweets_aapor$text[1:3]
tweets_aapor$text[1:10]
tweets_aapor$text[11:20]
# load packages
library(rvest)
library(stringr)
library(ggmap)
library(maps)
# scrape the page source
cities_string <- read_html("https://en.wikipedia.org/wiki/Berlin") %>%
# extract the list items
html_nodes(css = ".column-count-3 li") %>%
html_text()
# remove footnotes ([101], [102], ...)
cities_string <- str_replace(cities_string, "\\[\\d+\\]", "")
year <- str_extract(cities_string, "\\d{4}")
year
city <- str_extract(cities_string, "[[:alpha:] ]+") %>% str_trim
country <- str_extract(cities_string, "[[:alpha:] ]+$") %>% str_trim
city
country[1:5]
# scrape headlines from NY Times
url_p <- read_html("https://www.nytimes.com")
headlines <- html_nodes(url_p, css = ".story-heading")
headlines_raw <- html_text(headlines)
head(headlines_raw)
length(headlines_raw)
headlines_raw[1:10]
headlines_clean <- headlines_raw %>% str_replace_all("\\n", "") %>% str_trim()
length(headlines_clean)
str_detect(headlines_clean, "Trump") %>% table()
str_detect(headlines_clean, "Republican") %>% table()
str_detect(headlines_clean, "Democrat") %>% table()
str_detect(headlines_clean, "Obama") %>% table()
(141271+123880+104500)*(1/3)
((141271+123880+104500)*(1/3))/4
63000-(3*6500)
123880-30804
ggmap_credentials()
